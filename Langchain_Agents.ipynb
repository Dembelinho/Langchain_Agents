{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YA8HJ6yi7jay",
        "OTVIGWBB_J1V"
      ],
      "mount_file_id": "1DLKWkhf_4tsFUyIJNxnP5OLkgggkowyk",
      "authorship_tag": "ABX9TyPnkiqXaTcepmqghzG+rSlj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dembelinho/Langchain_Agents/blob/main/Langchain_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "YA8HJ6yi7jay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain_text_splitters"
      ],
      "metadata": {
        "id": "Pt6CUrQpf2YW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "e7q2rH7_Oi2D"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain"
      ],
      "metadata": {
        "id": "J9vQmZd1gNKU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet cohere"
      ],
      "metadata": {
        "id": "lqhecNO9S-qn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain-cohere"
      ],
      "metadata": {
        "id": "BAznzmCsgoaX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-6FgZOkfrW1",
        "outputId": "c15133ae-48ac-4e9e-8051-3d9abd170d34"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCQh6j9HDJo-",
        "outputId": "b88d4ea9-9771-4c28-bb6b-1549a53b39f2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q42meri_FWpv",
        "outputId": "b8770be9-618b-49cb-e67f-c9d9503ad57c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/171.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-fireworks"
      ],
      "metadata": {
        "id": "p6Gm5c5SHRQG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation"
      ],
      "metadata": {
        "id": "OTVIGWBB_J1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder"
      ],
      "metadata": {
        "id": "wQXHKYba_Ign"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "hrDb4gztfzjG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_cohere import ChatCohere"
      ],
      "metadata": {
        "id": "01jQ0LXRgW1q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Tools"
      ],
      "metadata": {
        "id": "BjqmWPSH7ndQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use two tools: Tavily (to search online) and then a retriever over a local index we will create"
      ],
      "metadata": {
        "id": "6hYDOzSh7BAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tavily’s Search API** is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed."
      ],
      "metadata": {
        "id": "uiciocEEKg9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "%env TAVILY_API_KEY=userdata.get('TAVILY_API_KEY') #put ur ravily api key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJLDViNB--bG",
        "outputId": "60bb04e9-99db-44f3-b0af-2e2baf08a0ca"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TAVILY_API_KEY=\"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\" #put ur ravily api key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search = TavilySearchResults()"
      ],
      "metadata": {
        "id": "7hjBQhTOyjJ0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://docs.cohere.com/docs/the-cohere-platform\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "UVaeqQLUO68w"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documents Loading"
      ],
      "metadata": {
        "id": "nkPXdwUvEAl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "loader1 = Docx2txtLoader(\"/content/drive/MyDrive/thingsboars.docx\") #path of ur docx\n",
        "docs1 = loader1.load()"
      ],
      "metadata": {
        "id": "kecRdUZNC0Xg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "#Embedding by cohere\n",
        "embeddings = CohereEmbeddings(cohere_api_key=userdata.get('COHERE_API_KEY')) # put ur cohere api key"
      ],
      "metadata": {
        "id": "_SK9KRN6Pdig"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "2SYq_MQjfxwO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings by HiggingFace\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "4SvrRiQeExp_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "WCK9w89XEmKg"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"FIREWORKS_API_KEY\"] =userdata.get('FIREWORKS_API_KEY')"
      ],
      "metadata": {
        "id": "Xw6bFgWMCnLA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohere_llm = ChatCohere(cohere_api_key=userdata.get('COHERE_API_KEY'))"
      ],
      "metadata": {
        "id": "2uam6pcFglf2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_fireworks import ChatFireworks\n",
        "mistral_llm = ChatFireworks(model=\"accounts/fireworks/models/mistral-7b-instruct-4k\")"
      ],
      "metadata": {
        "id": "lazwI9NsCqNQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"Répondez à la question suivante en vous basant uniquement sur le contexte fourni :\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(mistral_llm, prompt)"
      ],
      "metadata": {
        "id": "1_ikLPMSgAHt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_chain.invoke({\n",
        "    \"input\": \"La page d'accueil est divisée en plusieurs sections lesquels?\",\n",
        "    \"context\": [Document(page_content=\"page d'accueil\")]\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "jwSvLje2hdFe",
        "outputId": "be92bcfb-0f93-4ae3-cad5-f90026506ec6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"La page d'accueil est divisée en plusieurs sections qui peuvent inclure une section d'introduction, une section sur les produits ou services offerts, une section sur l'entreprise, une section sur les contacts et une section sur les liens vers les autres pages de l'entreprise.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **retriever** is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them."
      ],
      "metadata": {
        "id": "36hgcza1OAwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrievers accept a string query as input and return a list of Document's as output."
      ],
      "metadata": {
        "id": "6T_L91vOO6Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "bRj1n2a7hllf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"Cohere Large Language Models ?\"})\n",
        "print(response[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0TO61hOhn6k",
        "outputId": "adcb7244-6ea9-453b-8e46-9c09e02eed83"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohere Large Language Models (LLMs) are a set of pre-trained models that can be used to generate text, perform text classification, and perform text-based tasks such as summarization and chatbots. The Command family of models includes Command, Command R, and Command R+, which are used for text generation and conversational agents. The Rerank model is used to inject the intelligence of a language model into an existing search system. The Embed model improves the accuracy of search, classification, clustering, and RAG results. The Cohere Platform serves as a foundation for building LLM-powered applications, and the Chat endpoint can be used to build a conversational agent powered by the Command family of models. Retrieval-Augmented Generation (RAG) is a feature that allows an LLM to access external data sources, leading to better, more factual generations. Fine-tuning can be done with generative models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
        "])\n",
        "retriever_chain = create_history_aware_retriever(cohere_llm, retriever, prompt)"
      ],
      "metadata": {
        "id": "R4RtTQGliDO1"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can i use Cohere for embedding?\"), AIMessage(content=\"Yes!\")]\n",
        "retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzXNdThpiFVd",
        "outputId": "0f39fe51-1dd1-4e21-8102-35d332473fa2"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The Cohere Platform', metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'}),\n",
              " Document(page_content=\"Click here to learn more about Cohere foundation models.\\nThese LLMs Make it Easy to Build Conversational Agents (and Other LLM-powered Apps)\\nTry Coral to see what an LLM-powered conversational agent can look like. It is able to converse, summarize text, and write emails and articles.\\n\\nOur goal, however, is to enable you to build your own LLM-powered applications. The Chat endpoint, for example, can be used to build a conversational agent powered by the Command family of models. \\n\\nRetrieval-Augmented Generation (RAG)\\n“Grounding” refers to the practice of allowing an LLM to access external data sources – like the internet or a company’s internal technical documentation – which leads to better, more factual generations. \\nCoral is being used with grounding enabled in the screenshot below, and you can see how accurate and information-dense its reply is.\\n\\nWhat’s more, Coral’s advanced RAG capabilities allow you to see what underlying query the model generates when completing its tasks, and its output includes citations pointing you to where it found the information it uses. Both the query and the citations can be leveraged alongside the Cohere Embed and Rerank models to build a remarkably powerful RAG system, such as the one found in this guide.\\n\\nClick here to learn more about the Cohere serving platform.\\nUse Language Models to Build Better Search and RAG Systems\\nEmbeddings enable you to search based on what a phrase means rather than simply what keywords it contains, leading to search systems that incorporate context and user intent better than anything that has come before.\\n\\nLearn more about semantic search here.\\nCreate Fine-Tuned Models with Ease\\nTo create a fine-tuned model, simply upload a dataset and hold on while we train a custom model and then deploy it for you. Fine-tuning can be done with generative models, multi-label classification models, rerank models, and chat models.\\n\\nWhere you can access Cohere Models\\nDepending on your privacy/security requirements there are a number of ways to access Cohere:\\n\\nCohere API: this is the easiest option, simply grab an API key from the dashboard and start using the models hosted by Cohere.\\nCloud AI platforms:  this option offers a balance of ease-of-use and security. you can access Cohere on various cloud AI platforms such as Oracle's GenAI Service, AWS' Bedrock and Sagemaker platforms, Google Cloud, and Azure's AML service.\\nPrivate cloud deploy deployments: Cohere's models can be deployed privately in most virtual private cloud (VPC) environments, offering enhanced security and highest degree of customization. Please contact sales for information.\\n\\n\\nOn-Premise and Air Gapped Solutions\\n\\nOn-premise: if your organization deals with sensitive data that cannot live on a cloud we also offer the option for fully-private deployment on your own infrastructure. Please contact sales for information.\\n\\nLet us Know What You’re Making\\nWe hope this overview has whetted your appetite for building with our generative AI models. Reach out to us on Discord with any questions or to showcase your projects – we love hearing from the Cohere community!Updated 2 days ago Table of Contents\\n\\nCohere Large Language Models (LLMs).\\n\\n\\nThese LLMs Make it Easy to Build Conversational Agents (and Other LLM-powered Apps)\\n\\nRetrieval-Augmented Generation (RAG)\\nUse Language Models to Build Better Search and RAG Systems\\n\\n\\n\\nCreate Fine-Tuned Models with Ease\\n\\n\\nWhere you can access Cohere Models\\n\\nOn-Premise and Air Gapped Solutions\\nLet us Know What You’re Making\", metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'}),\n",
              " Document(page_content='Cohere Large Language Models (LLMs).\\nThe Command family of models includes Command, Command R, and Command R+. Together, they are the text-generation LLMs powering conversational agents, summarization, copywriting, and similar use cases. They work through the co.chat endpoint, which can be used with or without retrieval augmented generation RAG.\\nRerank is the fastest way to inject the intelligence of a language model into an existing search system. It can be accessed via the co.rerank endpoint.\\nEmbed improves the accuracy of search, classification, clustering, and RAG results. It also powers the co.embed and co.classify endpoints.', metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'}),\n",
              " Document(page_content='Case PatternsChaining PromptsValidating OutputsEvaluating OutputsConclusion - Prompt EngineeringModule 7: The Cohere PlatformServing PlatformFoundational ModelsEndpointsApplicationsConclusion - The Cohere PlatformModule 8: Retrieval-Augmented Generation (RAG)Appendix 1: NLP and ML FundamentalsHistory of NLPApplications of NLPText Pre-Processing in NLPHow to Convert Text Into VectorsPast Machine-Learning Methods of NLPHow to Build a ClassifierHow to Evaluate a ClassifierConclusion - NLPAppendix 2: Building AppsApp Examplescohere for aiCohere For AI Acceptable Use PolicyThe Cohere PlatformSuggest EditsCohere allows developers and enterprises to build LLM-powered applications. We do that by creating world-class models, and the supporting platform to deploy them securely and privately.', metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "document_chain = create_stuff_documents_chain(mistral_llm, prompt)\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
      ],
      "metadata": {
        "id": "34vWSxrDwAzI"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [HumanMessage(content=\"Can i use Cohere for embeddings?\"), AIMessage(content=\"Yes!\")]\n",
        "retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NglnDpHwwIi1",
        "outputId": "30d50112-c180-4988-f6c5-6ecc92a6b4b2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='Can i use Cohere for embeddings?'),\n",
              "  AIMessage(content='Yes!')],\n",
              " 'input': 'Tell me how',\n",
              " 'context': [Document(page_content='The Cohere Platform', metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'}),\n",
              "  Document(page_content=\"Click here to learn more about Cohere foundation models.\\nThese LLMs Make it Easy to Build Conversational Agents (and Other LLM-powered Apps)\\nTry Coral to see what an LLM-powered conversational agent can look like. It is able to converse, summarize text, and write emails and articles.\\n\\nOur goal, however, is to enable you to build your own LLM-powered applications. The Chat endpoint, for example, can be used to build a conversational agent powered by the Command family of models. \\n\\nRetrieval-Augmented Generation (RAG)\\n“Grounding” refers to the practice of allowing an LLM to access external data sources – like the internet or a company’s internal technical documentation – which leads to better, more factual generations. \\nCoral is being used with grounding enabled in the screenshot below, and you can see how accurate and information-dense its reply is.\\n\\nWhat’s more, Coral’s advanced RAG capabilities allow you to see what underlying query the model generates when completing its tasks, and its output includes citations pointing you to where it found the information it uses. Both the query and the citations can be leveraged alongside the Cohere Embed and Rerank models to build a remarkably powerful RAG system, such as the one found in this guide.\\n\\nClick here to learn more about the Cohere serving platform.\\nUse Language Models to Build Better Search and RAG Systems\\nEmbeddings enable you to search based on what a phrase means rather than simply what keywords it contains, leading to search systems that incorporate context and user intent better than anything that has come before.\\n\\nLearn more about semantic search here.\\nCreate Fine-Tuned Models with Ease\\nTo create a fine-tuned model, simply upload a dataset and hold on while we train a custom model and then deploy it for you. Fine-tuning can be done with generative models, multi-label classification models, rerank models, and chat models.\\n\\nWhere you can access Cohere Models\\nDepending on your privacy/security requirements there are a number of ways to access Cohere:\\n\\nCohere API: this is the easiest option, simply grab an API key from the dashboard and start using the models hosted by Cohere.\\nCloud AI platforms:  this option offers a balance of ease-of-use and security. you can access Cohere on various cloud AI platforms such as Oracle's GenAI Service, AWS' Bedrock and Sagemaker platforms, Google Cloud, and Azure's AML service.\\nPrivate cloud deploy deployments: Cohere's models can be deployed privately in most virtual private cloud (VPC) environments, offering enhanced security and highest degree of customization. Please contact sales for information.\\n\\n\\nOn-Premise and Air Gapped Solutions\\n\\nOn-premise: if your organization deals with sensitive data that cannot live on a cloud we also offer the option for fully-private deployment on your own infrastructure. Please contact sales for information.\\n\\nLet us Know What You’re Making\\nWe hope this overview has whetted your appetite for building with our generative AI models. Reach out to us on Discord with any questions or to showcase your projects – we love hearing from the Cohere community!Updated 2 days ago Table of Contents\\n\\nCohere Large Language Models (LLMs).\\n\\n\\nThese LLMs Make it Easy to Build Conversational Agents (and Other LLM-powered Apps)\\n\\nRetrieval-Augmented Generation (RAG)\\nUse Language Models to Build Better Search and RAG Systems\\n\\n\\n\\nCreate Fine-Tuned Models with Ease\\n\\n\\nWhere you can access Cohere Models\\n\\nOn-Premise and Air Gapped Solutions\\nLet us Know What You’re Making\", metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'}),\n",
              "  Document(page_content='Cohere Large Language Models (LLMs).\\nThe Command family of models includes Command, Command R, and Command R+. Together, they are the text-generation LLMs powering conversational agents, summarization, copywriting, and similar use cases. They work through the co.chat endpoint, which can be used with or without retrieval augmented generation RAG.\\nRerank is the fastest way to inject the intelligence of a language model into an existing search system. It can be accessed via the co.rerank endpoint.\\nEmbed improves the accuracy of search, classification, clustering, and RAG results. It also powers the co.embed and co.classify endpoints.', metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'}),\n",
              "  Document(page_content='Case PatternsChaining PromptsValidating OutputsEvaluating OutputsConclusion - Prompt EngineeringModule 7: The Cohere PlatformServing PlatformFoundational ModelsEndpointsApplicationsConclusion - The Cohere PlatformModule 8: Retrieval-Augmented Generation (RAG)Appendix 1: NLP and ML FundamentalsHistory of NLPApplications of NLPText Pre-Processing in NLPHow to Convert Text Into VectorsPast Machine-Learning Methods of NLPHow to Build a ClassifierHow to Evaluate a ClassifierConclusion - NLPAppendix 2: Building AppsApp Examplescohere for aiCohere For AI Acceptable Use PolicyThe Cohere PlatformSuggest EditsCohere allows developers and enterprises to build LLM-powered applications. We do that by creating world-class models, and the supporting platform to deploy them securely and privately.', metadata={'source': 'https://docs.cohere.com/docs/the-cohere-platform', 'title': 'The Cohere Platform', 'description': 'Cohere offers world-class Large Language Models (LLMs) like Command, Rerank, and Embed. These help developers and enterprises build LLM-powered applications such as conversational agents, summarization, and search systems. They also provide fine-tuning options and private, secure deployment choices.', 'language': 'en'})],\n",
              " 'answer': 'Cohere offers a variety of large language models (LLMs) that can be used for embeddings. These models are trained on massive amounts of text data and can be used to represent words, phrases, or even entire documents as vectors in a high-dimensional space.\\n\\nTo use Cohere for embeddings, you can access the `co.embed` endpoint, which takes in a piece of text and returns a vector representation of that text. You can also use the `co.classify` endpoint, which takes in a piece of text and a set of labels and returns the probability distribution over those labels.\\n\\nCohere also offers a variety of pre-trained models that are specifically designed for embeddings, such as the `Command` family of models. These models can be used to generate embeddings for a wide range of tasks, such as text classification, sentiment analysis, and named entity recognition.\\n\\nOverall, Cohere provides a powerful and'}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "1eue-j5PwSYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've so far created examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.\n",
        "\n",
        "One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access to two tools:\n",
        "\n",
        "1- The retriever we just created. This will let it easily answer questions about Cohere\n",
        "\n",
        "2- A search tool. This will let it easily answer questions that require up-to-date information.\n"
      ],
      "metadata": {
        "id": "_XIAaFtowZjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set up a tool for the retriever we just created\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"cohere_search\",\n",
        "    \"Search for information about Cohere. For any questions about Cohere, you must use this tool!\",\n",
        ")"
      ],
      "metadata": {
        "id": "LhISIWsJya0r"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can now create a list of the tools we want to work with\n",
        "\n",
        "tools = [retriever_tool, search]"
      ],
      "metadata": {
        "id": "muOLteFORCKs"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------ Learning and testing parts with tools made by user----------"
      ],
      "metadata": {
        "id": "78vJgWC6XrdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain"
      ],
      "metadata": {
        "id": "vaI9_2h-Xwgo"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "SAXaj5J0YPCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a tool\n",
        "we need to create a tool to call. For this example, we will create a custom tool from a function."
      ],
      "metadata": {
        "id": "vvc7aX15YwZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(first_int: int, second_int: int) -> int:\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "    return first_int * second_int"
      ],
      "metadata": {
        "id": "U81duperY5wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR_Bm8ZWZiKG",
        "outputId": "2af9b2af-c01a-4e93-ab93-b3e8592a617d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "multiply(first_int: int, second_int: int) -> int - Multiply two integers together.\n",
            "{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiply.invoke({\"first_int\": 4, \"second_int\": 5})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH08lg4GZkiW",
        "outputId": "2f6d9a90-3d10-4d66-d025-97b20d87b488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "%env COHERE_API_KEY=userdata.get('COHERE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ0PG1_Dc0Tn",
        "outputId": "b5a15ec4-ba70-40be-d18b-269e94b787ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: COHERE_API_KEY=userdata.get('COHERE_API_KEY')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm = ChatCohere(model=\"command-r\")"
      ],
      "metadata": {
        "id": "bub7a-Gicvlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = llm.bind_tools([multiply]) #We’ll use bind_tools to pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool when appropriate"
      ],
      "metadata": {
        "id": "4XGeZgMqnWVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg = llm_with_tools.invoke(\"whats 5 times forty two\")\n",
        "msg.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH0F55d8nZy9",
        "outputId": "61a7ac5b-28a2-4cd8-e996-fed11cf95154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply', 'args': {'first_int': 5, 'second_int': 42}, 'id': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoking the tool"
      ],
      "metadata": {
        "id": "YkHw2TJFpZQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | multiply\n",
        "chain.invoke(\"Qu'est-ce que quatre fois 23\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jraUl4F3oi4N",
        "outputId": "23eea3fe-5f58-494f-b461-8a42010a7764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "emU3sEjZ-SJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent"
      ],
      "metadata": {
        "id": "oapgn7Ln-Ux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain langchainhub"
      ],
      "metadata": {
        "id": "vYi-HkpqXJvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the prompt to use - can be replaced with any prompt that includes variables \"agent_scratchpad\" and \"input\"!\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "prompt.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Az58YCC2Mc",
        "outputId": "e9bd9d30-3e35-4501-eb43-0af113951158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "You are a helpful assistant\n",
            "\n",
            "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
            "\n",
            "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
            "\n",
            "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
            "\n",
            "\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def add(first_int: int, second_int: int) -> int:\n",
        "    \"Add two integers.\"\n",
        "    return first_int + second_int\n",
        "\n",
        "\n",
        "@tool\n",
        "def exponentiate(base: int, exponent: int) -> int:\n",
        "    \"Exponentiate the base to the exponent power.\"\n",
        "    return base**exponent"
      ],
      "metadata": {
        "id": "rgCU9ZkJDTFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [multiply, add, exponentiate]"
      ],
      "metadata": {
        "id": "pmEufktNDV9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the tool calling agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)"
      ],
      "metadata": {
        "id": "Q-JWWsN-DX5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "0g0FtGYUDcDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With an agent, we can ask questions that require arbitrarily-many uses of our tools"
      ],
      "metadata": {
        "id": "9S8U5gH9Die9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Take 3 to the fifth power and multiply that by the sum of twelve and three\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Q0SZP3aKDjIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------install ollama-------------"
      ],
      "metadata": {
        "id": "gTS1qr9lF__2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-7fBPcqGDRH",
        "outputId": "f7f00276-3598-4ae9-a68a-347e222e0e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama start\n",
        "!ollama run mistral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlM-cGRzHmCb",
        "outputId": "0398a113-9c04-4f5c-9c74-a8298a9ba94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time=2024-04-17T13:34:20.686Z level=INFO source=images.go:817 msg=\"total blobs: 0\"\n",
            "time=2024-04-17T13:34:20.687Z level=INFO source=images.go:824 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-04-17T13:34:20.687Z level=INFO source=routes.go:1143 msg=\"Listening on 127.0.0.1:11434 (version 0.1.32)\"\n",
            "time=2024-04-17T13:34:20.687Z level=INFO source=payload.go:28 msg=\"extracting embedded files\" dir=/tmp/ollama1873135486/runners\n",
            "time=2024-04-17T13:34:28.862Z level=INFO source=payload.go:41 msg=\"Dynamic LLM libraries [rocm_v60002 cpu cpu_avx cpu_avx2 cuda_v11]\"\n",
            "time=2024-04-17T13:34:28.862Z level=INFO source=gpu.go:121 msg=\"Detecting GPU type\"\n",
            "time=2024-04-17T13:34:28.862Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libcudart.so*\"\n",
            "time=2024-04-17T13:34:28.876Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: [/tmp/ollama1873135486/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.2.140]\"\n",
            "time=2024-04-17T13:34:28.877Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /tmp/ollama1873135486/runners/cuda_v11/libcudart.so.11.0: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-17T13:34:28.877Z level=INFO source=gpu.go:343 msg=\"Unable to load cudart CUDA management library /usr/local/cuda/lib64/libcudart.so.12.2.140: your nvidia driver is too old or missing, please upgrade to run ollama\"\n",
            "time=2024-04-17T13:34:28.877Z level=INFO source=gpu.go:268 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-04-17T13:34:28.884Z level=INFO source=gpu.go:314 msg=\"Discovered GPU libraries: []\"\n",
            "time=2024-04-17T13:34:28.884Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-17T13:34:28.884Z level=INFO source=routes.go:1164 msg=\"no GPU detected\"\n",
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create agent using google vertex"
      ],
      "metadata": {
        "id": "5JL6k6TNSbLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "hkvGMgBwSl9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-google-vertexai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psl8ldY8SzwP",
        "outputId": "50eee8d7-b6ca-4de3-d899-2d2a5148d024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project='MY-AGENTS-PROJECT')"
      ],
      "metadata": {
        "id": "2dHI_0iSUWF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "llm = ChatVertexAI(model=\"gemini-pro\")"
      ],
      "metadata": {
        "id": "2qTS6CJyS7Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the tool calling agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)"
      ],
      "metadata": {
        "id": "cISsuw_QUs09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "h9hiAPOSUvTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result\"\n",
        "    }\n",
        ")\n",
        "# you should activate vertexIA"
      ],
      "metadata": {
        "id": "9oXtaJFlU6CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------testing FIREWORKS IA-----------"
      ],
      "metadata": {
        "id": "2yzJdFtrYDR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU langchain-fireworks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHGfV4w_YXXh",
        "outputId": "ae2d5d8f-6b82-4604-8637-c9e1a40e6084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/292.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "MMWDROz5WP34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"FIREWORKS_API_KEY\"] =userdata.get('FIREWORKS_API_KEY')"
      ],
      "metadata": {
        "id": "V9tL6D4ZYLUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_fireworks import ChatFireworks\n",
        "\n",
        "llm = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")"
      ],
      "metadata": {
        "id": "IMUiYrZcYVbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the tool calling agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)"
      ],
      "metadata": {
        "id": "sGnErT9vYgcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an agent executor by passing in the agent and tools\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "tGzLgF8xYiz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK5ZEieuYkl3",
        "outputId": "30fae7c9-9bff-4964-a485-bd3854817bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mFirst, let's calculate 3 to the fifth power:\n",
            "\n",
            "3^5 = 243\n",
            "\n",
            "Next, let's find the sum of twelve and three:\n",
            "\n",
            "12 + 3 = 15\n",
            "\n",
            "Now, multiply the result by the fifth power of three:\n",
            "\n",
            "243 * 15 = 3645\n",
            "\n",
            "Finally, square the whole result:\n",
            "\n",
            "3645^2 = 13,292,025\n",
            "\n",
            "So, the result of taking 3 to the fifth power, multiplying that by the sum of twelve and three, and then squaring the whole result is 13,292,025.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result',\n",
              " 'output': \"First, let's calculate 3 to the fifth power:\\n\\n3^5 = 243\\n\\nNext, let's find the sum of twelve and three:\\n\\n12 + 3 = 15\\n\\nNow, multiply the result by the fifth power of three:\\n\\n243 * 15 = 3645\\n\\nFinally, square the whole result:\\n\\n3645^2 = 13,292,025\\n\\nSo, the result of taking 3 to the fifth power, multiplying that by the sum of twelve and three, and then squaring the whole result is 13,292,025.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"Porter 3 à la cinquième puissance et multiplier le résultat par la somme de douze et de trois, puis élever le tout au carré.\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaAqRuFfZxsQ",
        "outputId": "26194cd8-35ce-4189-ab8d-8e88e971fbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo solve this, we first calculate 3 to the 5th power:\n",
            "\n",
            "3^5 = 243\n",
            "\n",
            "Next, we find the sum of 12 and 3:\n",
            "\n",
            "12 + 3 = 15\n",
            "\n",
            "Then, we multiply the result by 243:\n",
            "\n",
            "243 * 15 = 3645\n",
            "\n",
            "Finally, we square the result:\n",
            "\n",
            "3645^2 = 13,282,025\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Porter 3 à la cinquième puissance et multiplier le résultat par la somme de douze et de trois, puis élever le tout au carré.',\n",
              " 'output': 'To solve this, we first calculate 3 to the 5th power:\\n\\n3^5 = 243\\n\\nNext, we find the sum of 12 and 3:\\n\\n12 + 3 = 15\\n\\nThen, we multiply the result by 243:\\n\\n243 * 15 = 3645\\n\\nFinally, we square the result:\\n\\n3645^2 = 13,282,025'}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}